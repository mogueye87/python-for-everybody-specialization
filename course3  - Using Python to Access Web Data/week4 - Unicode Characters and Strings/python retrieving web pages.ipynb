{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unicode characters and Strings"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# unicode -> est un emsemble de caractères, une liste de caractères qui ont chacun un nombre unique\n",
    "# utf-8 -> un encodage, un algorithme permettant de transformer un number en binare pour que l'ordinateur puisse le comprendre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "fhand = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "for line in fhand:\n",
    "    print(line.decode().rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 3\n",
      "{'But': 1, 'soft': 1, 'what': 1, 'light': 1, 'through': 1, 'yonder': 1, 'window': 1, 'breaks': 1, 'It': 1, 'is': 3, 'the': 3, 'east': 1, 'and': 3, 'Juliet': 1, 'sun': 2, 'Arise': 1, 'fair': 1, 'kill': 1, 'envious': 1, 'moon': 1, 'Who': 1, 'already': 1, 'sick': 1, 'pale': 1, 'with': 1, 'grief': 1}\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "fhand = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "counts = dict()\n",
    "bigCount = None\n",
    "bigWord = None\n",
    "for line in fhand:\n",
    "    line = line.decode().rstrip()\n",
    "    line = line.split()\n",
    "    for word in line:\n",
    "        counts[word] = counts.get(word, 0) + 1\n",
    "        if bigCount is None or counts[word] > bigCount:\n",
    "            bigCount = counts[word]\n",
    "            bigWord  = word\n",
    "print(bigWord, bigCount)\n",
    "print(counts)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter - http://www.dr-chuck.com/page1.htm\n",
      "http://www.dr-chuck.com/page2.htm\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "url = input(\"Enter - \")\n",
    "html = urllib.request.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "# retriev all or the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    print(tag.get('href', None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter - http://www.dr-chuck.com\n",
      "https://www.dr-chuck.com/csev-blog/\n",
      "https://www.si.umich.edu/\n",
      "https://www.ratemyprofessors.com/ShowRatings.jsp?tid=1159280\n",
      "https://www.dr-chuck.com/csev-blog/\n",
      "https://www.twitter.com/drchuck/\n",
      "https://www.dr-chuck.com/dr-chuck/resume/speaking.htm\n",
      "https://www.slideshare.net/csev\n",
      "/dr-chuck/resume/index.htm\n",
      "https://amzn.to/1K5Q81K\n",
      "https://www.coursera.org/instructor/drchuck\n",
      "http://afs.dr-chuck.com/papers/\n",
      "https://itunes.apple.com/us/podcast/computing-conversations/id731495760\n",
      "https://www.youtube.com/playlist?list=PLHJB2bhmgB7dFuY7HmrXLj5BmHGKTD-3R\n",
      "https://developers.imsglobal.org/\n",
      "https://www.youtube.com/user/csev\n",
      "https://vimeo.com/drchuck/videos\n",
      "https://backpack.openbadges.org/share/4f76699ddb399d162a00b89a452074b3/\n",
      "https://www.linkedin.com/in/charlesseverance/\n",
      "https://www.researchgate.net/profile/Charles_Severance/\n",
      "https://www.tsugicloud.org/\n",
      "/office\n",
      "https://www.coursera.org/course/learn/python\n",
      "https://www.coursera.org/specializations/web-applications/\n",
      "https://www.coursera.org/course/insidetheinternet\n",
      "http://www.py4e.com\n",
      "http://www.wa4e.com/\n",
      "http://www.dj4e.com/\n",
      "http://www.py4e.com/book\n",
      "http://www.pythonlearn.com/\n",
      "/sakai-book\n",
      "http://www.amazon.com/gp/product/1624311393/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=1624311393&linkCode=as2&tag=drchu02-20\n",
      "http://www.amazon.com/gp/product/059680069X/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=059680069X&linkCode=as2&tag=drchu02-20\n",
      "http://www.amazon.com/Performance-Computing-Architectures-Optimization-Benchmarks/dp/156592312X/\n",
      "http://oreilly.com/catalog/9781565923126/\n",
      "http://cnx.org/content/col11136/latest/\n",
      "https://www.sakaiproject.org/\n",
      "https://www.tsugi.org/\n",
      "https://developers.imsglobal.org/\n",
      "/obi-sample\n",
      "http://www.youtube.com/playlist?list=PLHJB2bhmgB7dFuY7HmrXLj5BmHGKTD-3R\n",
      "https://www.vimeo.com/17207620\n",
      "https://www.youtube.com/watch?v=BVKpW02hsrU\n",
      "https://www.youtube.com/watch?v=sa2WsgCvn7c\n",
      "https://www.vimeo.com/17213019\n",
      "https://www.youtube.com/watch?v=FJ078sO35M0\n",
      "http://afs.dr-chuck.com/citoolkit\n",
      "https://twitter.com/drchuck\n"
     ]
    }
   ],
   "source": [
    "# To run this, you can install BeautifulSoup\n",
    "# https://pypi.python.org/pypi/beautifulsoup4\n",
    "\n",
    "# Or download the file\n",
    "# http://www.py4e.com/code3/bs4.zip\n",
    "# and unzip it in the same directory as this file\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')\n",
    "html = urllib.request.urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    print(tag.get('href', None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'HTTP/1.1 400 Bad Request\\r\\nDate: Sun, 03 Mar 2019 20:51:34 GMT\\r\\nServer: Apache/2.4.18 (Ubuntu)\\r\\nContent-Length: 308\\r\\nConnection: close\\r\\nContent-Type: text/html; charset=iso-8859-1\\r\\n\\r\\n<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\\n<html><head>\\n<title>400 Bad Request</title>\\n</head><body>\\n<h1>Bad Request</h1>\\n<p>Your browser sent a request that this server could not understand.<br />\\n</p>\\n<hr>\\n<address>Apache/2.4.18 (Ubuntu) Server at do1.dr-chuck.com Port 80</address>\\n</body></html>\\n'\n",
      "HTTP/1.1 400 Bad Request\r\n",
      "Date: Sun, 03 Mar 2019 20:51:34 GMT\r\n",
      "Server: Apache/2.4.18 (Ubuntu)\r\n",
      "Content-Length: 308\r\n",
      "Connection: close\r\n",
      "Content-Type: text/html; charset=iso-8859-1\r\n",
      "\r\n",
      "<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\n",
      "<html><head>\n",
      "<title>400 Bad Request</title>\n",
      "</head><body>\n",
      "<h1>Bad Request</h1>\n",
      "<p>Your browser sent a request that this server could not understand.<br />\n",
      "</p>\n",
      "<hr>\n",
      "<address>Apache/2.4.18 (Ubuntu) Server at do1.dr-chuck.com Port 80</address>\n",
      "</body></html>\n",
      "\n",
      "b''\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mysock.connect(('data.pr4e.org', 80))\n",
    "cmd = 'GET http://data.pr4e.org/romeo.txt HTTP/1.0\\n\\n'.encode()\n",
    "mysock.send(cmd)\n",
    "\n",
    "while True:\n",
    "    data = mysock.recv(512)\n",
    "    print(data)\n",
    "    if (len(data) < 1):\n",
    "        break\n",
    "    print(data.decode())\n",
    "mysock.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n",
      "105\n",
      "110\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "print(ord('l'))\n",
    "print(ord('i'))\n",
    "print(ord('n'))\n",
    "print(ord('e'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.dr-chuck.com']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "tag = '<p>Please click <a href=\"http://www.dr-chuck.com\">here</a></p>'\n",
    "re.findall('href=\"(.+)\"', tag)\n",
    "#re.findall('.+', tag)\n",
    "#re.findall('<.*>', tag)\n",
    "#re.findall('http:.*', tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment week4: Web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')\n",
    "html = urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Retrieve all of the span tags\n",
    "tags = soup('span')\n",
    "sum = 0\n",
    "for tag in tags:\n",
    "    # Look at the parts of a tag\n",
    "    sum += int(tag.contents[0])\n",
    "    print('TAG:', tag)\n",
    "    print('URL:', tag.get('span', None))\n",
    "    print('Contents:', tag.contents[0])\n",
    "    print('Attrs:', tag.attrs)\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "with urllib.request.urlopen('http://python.org/') as response:\n",
    "    html = response.read()\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4.2: BeautifulSoup parsing web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter - http://py4e-data.dr-chuck.net/known_by_Ngonidzashe.html\n",
      "Enter count: 7\n",
      "Enter position: 18\n",
      "http://py4e-data.dr-chuck.net/known_by_Ebony.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Maksim.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Shai.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Lakshya.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Kamilah.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Derry.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Etiene.html\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "# prompting the user  to enter the url for web scraping\n",
    "url = input('Enter - ')\n",
    "# user input for repeating the process\n",
    "count = int(input('Enter count: '))\n",
    "# user input for the position of the url to be extract\n",
    "position = int(input('Enter position: '))\n",
    "\n",
    "i = 0\n",
    "while i < count:\n",
    "    html = urllib.request.urlopen(url, context=ctx).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    # Retrieve all of the anchor tags\n",
    "    tags = soup('a')\n",
    "    url = tags[position - 1].get('href', None)\n",
    "    print(url)\n",
    "    i +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
